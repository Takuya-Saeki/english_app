{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大文字小文字を、小文字に統一する\n",
    "textの前処理で、基本形で保持する\n",
    "nltkで、できそう\n",
    "\n",
    "中学生用の単語リストを英検5級,4級、3級も含める\n",
    "数字、月など、明示的ではないけど中学生が知っている単語を追加していく\n",
    "数字も中学生のリストに含めとく\n",
    "\n",
    "\n",
    "それでも厳しい気がするので、高校受験用の文章を生成するときは、センター試験用の単語リストの単語を含める。\n",
    "ユーザーが選んだ単語を自動で単語リストに入れれば良い、その意味はgptに聞くのが無難？\n",
    "\n",
    "iphoneでも、選択した単語の意味を調べたり翻訳できる。そのデータをjavascriptに入れて表示できる？\n",
    "\n",
    "固有名詞の判定は、spacyというライブラリでできそう\n",
    "\n",
    "\n",
    "型ヒントをつけて、関数にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大学受験レベルの英文を生成して、どれくらい難しい単語が含まれているかを見る\n",
    "\n",
    "高校受験レベルは、中学3年生と高校1年生レベルの英単語をメインで使って生成したらいけそう\n",
    "\n",
    "知らん単語があれば自分で単語帳に登録すれば良い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_none_testwords_in_text(file_path: str, text: set) -> set:\n",
    "    # 単語を格納するリストを初期化\n",
    "    words_for_test = set()\n",
    "\n",
    "    # CSVファイルを開く\n",
    "    with open(file_path, newline=\"\") as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # 各行を読み込み、単語をリストに追加\n",
    "        for row in reader:\n",
    "            # rowはリスト形式なので、最初の要素を取得\n",
    "            word = row[0]\n",
    "            words_for_test.add(word)\n",
    "\n",
    "    # words_for_testに含まれていない単語を、textから抜き出す\n",
    "    not_included = text - words_for_test\n",
    "    return not_included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大文字を小文字に変換、単語を基本形に変換\n",
    "def pre_process(text:str) -> set:\n",
    "    text_lowew = text.lower()\n",
    "    # テキストを単語に分割\n",
    "    words_in_text = text_lowew.split()\n",
    "    # レンマタイザの初期化\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # textの単語の基本形を取得\n",
    "    lemmatized_words_in_text  = [lemmatizer.lemmatize(word) for word in words_in_text]\n",
    "\n",
    "    return set(lemmatized_words_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['professional', 'baseball', 'team', 'manager', 'and', 'player', 'have', 'their', 'schedule', 'set', 'month', 'before', 'the', 'season', 'starts.', 'this', 'includes', 'game', 'date', 'and', 'time', 'for', 'flights.', 'the', 'season', 'begin', 'in', 'april,', 'but', 'they', 'have', 'camp', 'in', 'february', 'and', 'open', 'game', 'in', 'march.', 'so,', 'from', 'february', 'to', 'about', 'october,', 'their', 'schedule', 'are', 'mostly', 'planned.', 'i', 'usually', 'take', 'a', 'few', 'day', 'off', 'in', 'december', 'and', 'do', 'my', 'own', 'training', 'in', 'january.', 'on', 'a', 'game', 'day,', 'like', 'when', 'we', 'play', 'at', 'jingu', 'stadium,', 'i', 'wake', 'up', 'at', '10', 'am,', 'get', 'to', 'the', 'stadium', 'after', 'noon,', 'start', 'practice', 'around', '2', 'pm,', 'and', 'the', 'game', 'begin', 'at', '6', 'pm.', 'after', 'the', 'game,', 'i', 'discus', 'it', 'with', 'the', 'coach', 'and', 'plan', 'for', 'the', 'next', 'game.', 'then,', 'i', 'do', 'some', 'training', 'and', 'go', 'home.', 'i', 'usually', 'get', 'home', 'by', 'midnight', 'and', 'sleep', 'at', '3', 'am.', 'we', 'have', 'six', 'game', 'a', 'week', 'and', 'travel', 'too,', 'so', 'we', 'only', 'get', 'about', 'two', 'day', 'off', 'each', 'month.']\n"
     ]
    }
   ],
   "source": [
    "# CSVファイルのパス\n",
    "# file_path = '../word_list/csv_folder/word_list_2zi_test.csv'\n",
    "file_path = \"../word_list/csv_folder/word_list_j_high_school.csv\"\n",
    "\n",
    "text = \"Professional baseball team managers and players have their schedules set months before the season starts. This includes game dates and times for flights. The season begins in April, but they have camps in February and open games in March. So, from February to about October, their schedules are mostly planned. I usually take a few days off in December and do my own training in January. On a game day, like when we play at Jingu Stadium, I wake up at 10 AM, get to the stadium after noon, start practice around 2 PM, and the game begins at 6 PM. After the game, I discuss it with the coaches and plan for the next game. Then, I do some training and go home. I usually get home by midnight and sleep at 3 AM. We have six games a week and travel too, so we only get about two days off each month.\"\n",
    "\n",
    "text_lowew = text.lower()\n",
    "# テキストを単語に分割\n",
    "words_in_text = text_lowew.split()\n",
    "# レンマタイザの初期化\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# textの単語の基本形を取得\n",
    "lemmatized_words_in_text  = [lemmatizer.lemmatize(word) for word in words_in_text]\n",
    "\n",
    "print(lemmatized_words_in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['professional', 'baseball', 'team', 'manager', 'and', 'player', 'have', 'their', 'schedule', 'set', 'month', 'before', 'the', 'season', 'start', '.', 'this', 'include', 'game', 'date', 'and', 'time', 'for', 'flight', '.', 'the', 'season', 'begin', 'in', 'April', ',', 'but', 'they', 'have', 'camp', 'in', 'February', 'and', 'open', 'game', 'in', 'March', '.', 'so', ',', 'from', 'February', 'to', 'about', 'October', ',', 'their', 'schedule', 'be', 'mostly', 'plan', '.', 'I', 'usually', 'take', 'a', 'few', 'day', 'off', 'in', 'December', 'and', 'do', 'my', 'own', 'training', 'in', 'January', '.', 'on', 'a', 'game', 'day', ',', 'like', 'when', 'we', 'play', 'at', 'Jingu', 'Stadium', ',', 'I', 'wake', 'up', 'at', '10', 'am', ',', 'get', 'to', 'the', 'stadium', 'after', 'noon', ',', 'start', 'practice', 'around', '2', 'pm', ',', 'and', 'the', 'game', 'begin', 'at', '6', 'pm', '.', 'after', 'the', 'game', ',', 'I', 'discuss', 'it', 'with', 'the', 'coach', 'and', 'plan', 'for', 'the', 'next', 'game', '.', 'then', ',', 'I', 'do', 'some', 'training', 'and', 'go', 'home', '.', 'I', 'usually', 'get', 'home', 'by', 'midnight', 'and', 'sleep', 'at', '3', 'am', '.', 'we', 'have', 'six', 'game', 'a', 'week', 'and', 'travel', 'too', ',', 'so', 'we', 'only', 'get', 'about', 'two', 'day', 'off', 'each', 'month', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or other models\n",
    "doc = nlp(text)\n",
    "lemmatized_sentence = [token.lemma_ for token in doc]\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'january.', 'then,', 'game.', 'camp', 'home.', 'day,', 'game,', 'home', 'planned.', 'march.', 'flights.', 'am,', 'training', 'team', 'schedule', 'am.', 'jingu', 'october,', 'pm.', '6', 'discus', 'stadium,', 'too,', '2', 'month.', 'april,', 'coach', '3', 'noon,', 'starts.', 'professional', 'includes', '10', 'so,', 'pm,'}\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "a = check_none_testwords_in_text(file_path, text_pre_process)\n",
    "print(a)\n",
    "print(len(set(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熟語を抜き出す例\n",
    "\n",
    "熟語リストを作って、textに含まれているかをリスト内包表記で書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found idioms: ['break the ice', 'hit the books']\n"
     ]
    }
   ],
   "source": [
    "idioms = [\"break the ice\", \"hit the books\", \"let the cat out of the bag\", \"at the drop of a hat\"]\n",
    "\n",
    "# 解析するテキスト\n",
    "text = \"I need to hit the books for my exam, but it's hard to get started. Maybe a study group would help break the ice.\"\n",
    "\n",
    "# 熟語を検索して抽出\n",
    "found_idioms = [idiom for idiom in idioms if idiom in text]\n",
    "\n",
    "print(\"Found idioms:\", found_idioms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本形をnltkから取得して、基本形で見比べる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "固有名詞の判定をするコード\n",
    "\n",
    "固有名詞の範囲が広すぎる気がする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Apple' is a proper noun\n",
      "'is' is not a proper noun\n",
      "'looking' is not a proper noun\n",
      "'at' is not a proper noun\n",
      "'buying' is not a proper noun\n",
      "'U.K.' is a proper noun\n",
      "'startup' is not a proper noun\n",
      "'for' is not a proper noun\n",
      "'$' is not a proper noun\n",
      "'1' is not a proper noun\n",
      "'billion' is not a proper noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# spaCyの英語モデルをロード\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 解析したい文章\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# テキストを処理\n",
    "doc = nlp(text)\n",
    "\n",
    "# 各単語について、固有名詞かどうかを判定\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        print(f\"'{token.text}' is a proper noun\")\n",
    "    else:\n",
    "        print(f\"'{token.text}' is not a proper noun\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文の終わりがわからない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break the ice hit the books let the cat out of the bag at the drop of a hat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "idioms = [\"break the ice\", \"hit the books\", \"let the cat out of the bag\", \"at the drop of a hat\"]\n",
    "\n",
    "# リスト内の熟語を一つの文字列に連結\n",
    "combined_string = ' '.join(idioms)\n",
    "\n",
    "print(combined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
