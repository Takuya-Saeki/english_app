{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題点\n",
    "- ~~ 基本形に直せきれていない ~~ \n",
    "- 単語を指定して、その範囲内で翻訳できるか確認する\n",
    "- ~~ 固有名詞が未知の単語と判定されてしまう~~ \n",
    "    - ~~ 固有表現認識を使う　https://qiita.com/Hironsan/items/a5acf1d121926666907b~~ \n",
    "\n",
    "確認すべきこと\n",
    "- pタグを取得することで、大半のURLから本文を取得できるか確認する\n",
    "- 決済のapiをどうするか考える\n",
    "    - paypalか、urlを埋め込むだけのタイプもあるらしい\n",
    "    - 個人用かビジネス用か\n",
    "\n",
    "仕様\n",
    "- 知らん単語があれば自分で単語帳に登録すれば良い\n",
    "- 熟語は無視する。意味のわからない単語は自分で調べるだろうから\n",
    "- 引用符をつけずに英訳するように指定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacyで、レンマ化するのがおすすめらしい。\n",
    "\n",
    "基本形に変換するレンマ化の精度を高める\n",
    "\n",
    "固有表現抽出もspacyを使うのが良さそう\n",
    "### 下で性能が良いのを確認できた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# python3 -m spacy download en_core_web_sm を実行する必要あるかも\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or other models\n",
    "\n",
    "# # 大文字を小文字に変換、単語を基本形に変換\n",
    "# def pre_process(text: str) -> set[str]:\n",
    "#     text = text.replace('.', '')  # ピリオドを取り除く\n",
    "#     text = text.replace(',', '')  # コンマを取り除く\n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "#     text_lowew = text.lower()\n",
    "#     # テキストを単語に分割\n",
    "#     words_in_text = text_lowew.split()\n",
    "#     # レンマタイザの初期化\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     # textの単語の基本形を取得\n",
    "#     lemmatized_words_in_text = [lemmatizer.lemmatize(word) for word in words_in_text]\n",
    "\n",
    "#     return set(lemmatized_words_in_text)\n",
    "\n",
    "# 前処理した単語の集合と、受験で覚えるべき単語の集合を比べる\n",
    "def check_none_testwords_in_text(file_path: str, text: set[str]) -> set[str]:\n",
    "    # 単語を格納するリストを初期化\n",
    "    words_for_test = set()\n",
    "\n",
    "    # CSVファイルを開く\n",
    "    with open(file_path, newline=\"\") as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # 各行を読み込み、単語をリストに追加\n",
    "        for row in reader:\n",
    "            # rowはリスト形式なので、最初の要素を取得\n",
    "            word = row[0]\n",
    "            words_for_test.add(word)\n",
    "\n",
    "    # words_for_testに含まれていない単語を、textから抜き出す\n",
    "    not_included = text - words_for_test\n",
    "    return not_included\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大文字小文字の変換、固有名詞の除去をどの順番でするか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有名詞を文中から抽出,大文字を小文字に変換、単語を基本形に変換\n",
    "def pre_process(text:str)->set[str]:\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "\n",
    "    # 基本形に変換、すでに小文字化して返す、固有名詞は大文字のまま\n",
    "    lemmatized_sentence = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    lemma_extract_propn = set(lemmatized_sentence)-set(proper_nouns)-{word.lower() for word in proper_nouns}\n",
    "    return lemma_extract_propn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上の二つの関数を合わせる\n",
    "def set_none_words(file_path: str, text: set[str]):\n",
    "    text_pre_process = pre_process(text)\n",
    "    answer = check_none_testwords_in_text(file_path, text_pre_process)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高校受験用の単語集合と比較 17 {'conscious', 'drunk', 'alright', 'incident', 'troublesome', 'timing', 'elicit', 'burst', 'co', 'boss', 'policewoman', 'reveal', 'mention', 'collapse', 'revelation', 'acquaintance', 'predict'}\n",
      "大学受験用の単語集合と比較 8 {'drunk', 'alright', 'timing', 'elicit', 'co', 'boss', 'policewoman', 'troublesome'}\n"
     ]
    }
   ],
   "source": [
    "# CSVファイルのパス\n",
    "file_path_high = \"../word_list/csv_folder/word_list_j_high_school.csv\"\n",
    "file_path_uni = \"../word_list/csv_folder/word_list_2zi_test.csv\"\n",
    "\n",
    "# text = \"Professional baseball team managers and players have their schedules set months before the season starts. This includes game dates and times for flights. The season begins in April, but they have camps in February and open games in March. So, from February to about October, their schedules are mostly planned. I usually take a few days off in December and do my own training in January. On a game day, like when we play at Jingu Stadium, I wake up at 10 AM, get to the stadium after noon, start practice around 2 PM, and the game begins at 6 PM. After the game, I discuss it with the coaches and plan for the next game. Then, I do some training and go home. I usually get home by midnight and sleep at 3 AM. We have six games a week and travel too, so we only get about two days off each month.\"\n",
    "text = \"In explaining why he predicted 3rd place for Daigo, he mentioned there might be opinions like it's troublesome if a boss gets drunk. He then revealed that recently, an acquaintance of mine was passing by the police box at Shinagawa Station late at night. He saw someone collapsed there, and a policewoman was asking if they were alright. When he looked closely, it turned out to be Daigo. He's still doing things like that.Daigo burst into laughter at this unexpected revelation. The co-performers couldn't hide their surprise, and Hakata Daikichi asked about the timing of the incident, thinking it was a story from years ago. But Yamauchi immediately replied that it was just two weeks ago. Daigo then elicited laughter by explaining that he is a bit conscious that if he's going to collapse, it's better near a police box where someone will help him.\"\n",
    "\n",
    "print(\n",
    "    \"高校受験用の単語集合と比較\",\n",
    "    len(set_none_words(file_path_high, text)),\n",
    "    set_none_words(file_path_high, text),\n",
    ")\n",
    "print(\n",
    "    \"大学受験用の単語集合と比較\",\n",
    "    len(set_none_words(file_path_uni, text)),\n",
    "    set_none_words(file_path_uni, text),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hakata', 'Daigo', 'Shinagawa', 'Yamauchi', 'Station', 'Daikichi'}\n"
     ]
    }
   ],
   "source": [
    "def pick_propm(text:str)->set[str]:\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "    return set(proper_nouns)\n",
    "\n",
    "print(pick_propm(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alright', 'burst', 'box', 'revelation', 'might', 'he', 'performer', 'just', 'Yamauchi', 'acquaintance', 'unexpected', '-', 'explain', 'see', 'recently', 'police', 'night', 'like', 'late', 'an', 'week', 'it', 'when', 'where', 'from', 'about', 'mine', 'conscious', 'thing', 'will', 'mention', 'Daikichi', 'two', 'incident', 'help', 'near', 'if', '.', 'think', 'place', ',', 'a', 'boss', 'surprise', 'be', 'immediately', 'pass', 'Station', 'the', 'they', 'do', 'Shinagawa', 'then', 'predict', 'their', 'drunk', 'timing', 'into', 'year', 'reveal', 'and', 'go', 'this', 'story', 'collapse', 'still', 'in', 'for', '3rd', 'co', 'Hakata', 'of', 'elicit', 'that', 'why', 'well', 'ask', 'could', 'ago', 'reply', 'opinion', 'someone', 'at', 'get', 'closely', 'there', 'Daigo', 'daigo', 'by', 'to', 'laughter', 'troublesome', 'turn', 'look', 'out', 'not', 'policewoman', 'hide', 'bit', 'but'}\n"
     ]
    }
   ],
   "source": [
    "def lem(text:str)->set[str]:\n",
    "    doc = nlp(text)\n",
    "    lemmatized_sentence = [token.lemma_ for token in doc]\n",
    "    return set(lemmatized_sentence)\n",
    "print(lem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
