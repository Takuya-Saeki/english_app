{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# python3 -m spacy download en_core_web_sm を実行する必要あるかも\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or other models\n",
    "\n",
    "\n",
    "# 固有名詞を文中から抽出,大文字を小文字に変換、単語を基本形に変換\n",
    "def pre_process(text: str) -> set[str]:\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "\n",
    "    # 基本形に変換、すでに小文字化して返す、固有名詞は大文字のまま\n",
    "    lemmatized_sentence = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    lemma_extract_propn = (\n",
    "        set(lemmatized_sentence)\n",
    "        - set(proper_nouns)\n",
    "        - {word.lower() for word in proper_nouns}\n",
    "    )\n",
    "    return lemma_extract_propn\n",
    "\n",
    "\n",
    "# 前処理した単語の集合と、受験で覚えるべき単語の集合を比べる\n",
    "def check_none_testwords_in_text(file_path: str, text: set[str]) -> set[str]:\n",
    "    # 単語を格納するリストを初期化\n",
    "    words_for_test = set()\n",
    "\n",
    "    # CSVファイルを開く\n",
    "    with open(file_path, newline=\"\") as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # 各行を読み込み、単語をリストに追加\n",
    "        for row in reader:\n",
    "            # rowはリスト形式なので、最初の要素を取得\n",
    "            word = row[0]\n",
    "            words_for_test.add(word)\n",
    "\n",
    "    # words_for_testに含まれていない単語を、textから抜き出す\n",
    "    not_included = text - words_for_test\n",
    "    return not_included\n",
    "\n",
    "\n",
    "# 上の二つの関数を合わせる\n",
    "def set_none_words(file_path: str, text: set[str]):\n",
    "    text_pre_process = pre_process(text)\n",
    "    answer = check_none_testwords_in_text(file_path, text_pre_process)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高校受験用の単語集合と比較 7 {'okay', 'collapse', 'boss', 'drunk', 'awareness', 'co', 'reveal'}\n",
      "高校受験用の単語集合と比較 5 {'okay', 'boss', 'drunk', 'co', 'reveal'}\n",
      "大学受験用の単語集合と比較 4 {'drunk', 'boss', 'co', 'okay'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CSVファイルのパス\n",
    "file_path_high = \"../word_list/csv_folder/word_list_j_high_school.csv\"\n",
    "file_path_center = \"../word_list/csv_folder/word_list_center_test.csv\"\n",
    "file_path_uni = \"../word_list/csv_folder/word_list_2zi_test.csv\"\n",
    "\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(\"input_text.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\n",
    "    \"高校受験用の単語集合と比較\",\n",
    "    len(set_none_words(file_path_high, text)),\n",
    "    set_none_words(file_path_high, text),\n",
    ")\n",
    "print(\n",
    "    \"高校受験用の単語集合と比較\",\n",
    "    len(set_none_words(file_path_center, text)),\n",
    "    set_none_words(file_path_center, text),\n",
    ")\n",
    "print(\n",
    "    \"大学受験用の単語集合と比較\",\n",
    "    len(set_none_words(file_path_uni, text)),\n",
    "    set_none_words(file_path_uni, text),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hakata', 'Station', 'Yamauchi', 'Daikichi', 'Shinagawa', 'Daigo'}\n"
     ]
    }
   ],
   "source": [
    "def pick_propm(text:str)->set[str]:\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "    return set(proper_nouns)\n",
    "\n",
    "print(pick_propm(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'make', 'a', 'star', 'share', 'to', 'old', 'will', 'ago', 'collapse', 'explain', 'have', 'his', 'friend', 'say', 'officer', 'such', 'recently', 'happen', 'boss', 'third', 'story', 'do', ',', 'reveal', 'Daigo', 'an', 'Hakata', 'when', 'guess', 'look', 'night', 'it', 'immediately', 'problem', 'like', 'help', 'two', '\"', 'Yamauchi', 'station', 'there', 'you', 'the', 'be', 'opinion', 'week', 'drunk', 'might', 'get', 'close', 'answer', 'on', 'they', 'of', 'ask', 'Shinagawa', 'laugh', 'no', 'co', '-', 'if', 'still', 'well', 'think', 'surely', 'Station', 'he', 'late', '\\n\\n', 'okay', '.', 'that', 'see', 'surprised', 'Daikichi', 'near', 'also', 'at', 'then', 'go', 'awareness', 'and', 'bit', 'this', 'unexpected', '?', 'thing', 'someone', 'police', 'by'}\n"
     ]
    }
   ],
   "source": [
    "def lem(text:str)->set[str]:\n",
    "    doc = nlp(text)\n",
    "    lemmatized_sentence = [token.lemma_ for token in doc]\n",
    "    return set(lemmatized_sentence)\n",
    "print(lem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
